{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8045a6cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as t\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import random\n",
    "import re\n",
    "import csv\n",
    "import tqdm\n",
    "import torchaudio\n",
    "\n",
    "import librosa\n",
    "import soundfile\n",
    "from ast import literal_eval\n",
    "from transformers import AutoProcessor, WhisperForConditionalGeneration\n",
    "from datasets import load_dataset, DatasetDict, Audio\n",
    "from pydub import AudioSegment\n",
    "\n",
    "device = t.device(\"mps\" if t.backends.mps.is_available() else \"cuda\" if t.cuda.is_available() else \"cpu\")\n",
    "model_id = \"openai/whisper-small\"\n",
    "sr=16000\n",
    "random.seed(100) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7533b7b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = AutoProcessor.from_pretrained(model_id)\n",
    "model = WhisperForConditionalGeneration.from_pretrained(model_id).to(device)\n",
    "model.generation_config.language = \"english\"\n",
    "model.generation_config.task = \"transcribe\"\n",
    "processor.feature_extractor.return_attention_mask = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "159beb1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def audiosegment_to_array(seg, target_sr=sr):\n",
    "    samples = np.array(seg.get_array_of_samples())\n",
    "    max_val = float(1 << (8 * seg.sample_width - 1))\n",
    "    y = samples.astype(np.float32) / max_val\n",
    "    if seg.frame_rate != target_sr:\n",
    "        y = librosa.resample(y, orig_sr=seg.frame_rate, target_sr=target_sr)\n",
    "    return y\n",
    "\n",
    "def transcribe_batch(segments, processor, model, sr=16000, device=None, batch_size=8):\n",
    "    model.eval()\n",
    "    texts = []\n",
    "    for i in range(0, len(segments), batch_size):\n",
    "        chunk = segments[i:i+batch_size]\n",
    "        arrays = [audiosegment_to_array(seg, target_sr=sr) for seg in chunk]\n",
    "        inputs = processor(arrays, sampling_rate=sr, return_tensors=\"pt\", padding=True)\n",
    "        input_features = inputs[\"input_features\"].to(device)\n",
    "        attention_mask = inputs[\"attention_mask\"].to(device)\n",
    "        gen_ids = model.generate(input_features=input_features, attention_mask=attention_mask, task=\"transcribe\")  \n",
    "        texts.extend(processor.batch_decode(gen_ids, skip_special_tokens=True))\n",
    "    return [t.strip() for t in texts]\n",
    "\n",
    "\n",
    "def create_trial(condition, target, frames, talker, n_continuum):\n",
    "    endpoints = target.split(\"_\")\n",
    "    frames = [f for f in frames if endpoints[0] not in f and endpoints[1] not in f]\n",
    "    random.shuffle(frames)\n",
    "\n",
    "    silence = AudioSegment.silent(duration=500)\n",
    "    combined = AudioSegment.silent(duration=0)\n",
    "    for f in frames:\n",
    "        audio = AudioSegment.from_wav(f'audio/MP/{talker}/{condition}/{f}')\n",
    "        combined += audio + silence\n",
    "    \n",
    "    trials = [combined + AudioSegment.from_wav(f'audio/MP/{talker}/continuum/{target}_1_{i}.wav') + silence for i in range(n_continuum)]\n",
    "    return trials, frames\n",
    "\n",
    "def create_filler(target, frames, talker, n_continuum):\n",
    "    silence = AudioSegment.silent(duration=500)\n",
    "    combined = AudioSegment.silent(duration=0)\n",
    "    for f in frames:\n",
    "        audio = AudioSegment.from_wav(f'audio/fillers/{talker}/{f}')\n",
    "        combined += audio + silence\n",
    "\n",
    "    trials = [combined + AudioSegment.from_wav(f'audio/MP/{talker}/continuum/{target}_1_{i}.wav') + silence for i in range(n_continuum)]\n",
    "\n",
    "    return trials, frames\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e9b55dd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300/300 [55:53<00:00, 11.18s/it] \n"
     ]
    }
   ],
   "source": [
    "def exp(output_path, talkers, n_continuum, n_trials):\n",
    "    with open(output_path, \"w\", newline=\"\") as f:\n",
    "        # Prep csv\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow([\"id\", \"talker\", \"condition\", \"context\", \"transcript\", \"target\", \"target_step\"])\n",
    "        # Get possible targets\n",
    "        for t in talkers:\n",
    "\n",
    "            # Get all the audio paths\n",
    "            targets = os.listdir(f'audio/MP/{t}/continuum')\n",
    "            targets = list(set([re.search(r'([a-zA-Z]+_[a-zA-Z]+)_[0-9]', target).group(1) for target in targets]))\n",
    "            voiced = os.listdir(f'audio/MP/{t}/voiced')\n",
    "            voiced = [v for v in voiced if v != '.DS_Store']\n",
    "            voiceless = os.listdir(f'audio/MP/{t}/voiceless')\n",
    "            voiceless = [v for v in voiceless if v != '.DS_Store']\n",
    "            fillers = os.listdir(f'audio/fillers/{t}')\n",
    "            fillers = [f for f in fillers if f != '.DS_Store']\n",
    "\n",
    "            # A \"trial\" here represents every time a target is selected. Iterate throught the three conditions and every step of the continuum for each.\n",
    "            for n in tqdm.tqdm(range(n_trials)):\n",
    "                target = targets[n % len(targets)]\n",
    "\n",
    "                for c in ['voiced', 'voiceless', 'control']:\n",
    "                    if c == 'voiced':\n",
    "                        audio, frames = create_trial(c, target, voiced, t, n_continuum)\n",
    "                    elif c == 'voiceless':\n",
    "                        audio, frames = create_trial(c, target, voiceless, t, n_continuum)\n",
    "                    else:\n",
    "                        random.shuffle(fillers)\n",
    "                        control_frames = fillers[:len(voiced)-1]\n",
    "                        audio, frames = create_filler(target, control_frames, t, n_continuum)\n",
    "\n",
    "                    transcript = transcribe_batch(audio, processor, model, sr, device, batch_size=n_continuum)\n",
    "                    for i in range(n_continuum):\n",
    "                        \n",
    "                        row = [n, t, c, frames, transcript[i], target, i]\n",
    "                        writer.writerow(row)\n",
    "\n",
    "exp('data/SelAd.csv', ['hope'], 13, 300)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "perc_learn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
