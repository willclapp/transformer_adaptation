{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4e0d08d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as t\n",
    "import random, re, csv, tqdm, os\n",
    "import numpy as np\n",
    "from transformers import AutoProcessor, WhisperForConditionalGeneration\n",
    "from util import load_to_array, transcribe_batch, TrialData, WordBoundary\n",
    "import pickle\n",
    "\n",
    "device = t.device(\"cuda\" if t.cuda.is_available() else \"mps\" if t.backends.mps.is_available() else \"cpu\")\n",
    "model_id = \"openai/whisper-small\"\n",
    "sr=16000\n",
    "random.seed(100) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "386cc95e",
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = AutoProcessor.from_pretrained(model_id)\n",
    "model = WhisperForConditionalGeneration.from_pretrained(model_id).to(device)\n",
    "model.generation_config.language = \"english\"\n",
    "model.generation_config.task = \"transcribe\"\n",
    "processor.feature_extractor.return_attention_mask = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca438e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def select_frames(target, voiced, voiceless, talker):\n",
    "    endpoints = target.split(\"_\")\n",
    "    voiced_frames = [f for f in voiced if endpoints[0] not in f]\n",
    "    voiceless_frames = []\n",
    "    control_frames = []\n",
    "    control_condition_order = []\n",
    "    random.shuffle(voiced)\n",
    "    neutral_frames = [re.search(r\"([a-z]+-[0-9]{2})\", f).group(1) for f in voiced_frames]\n",
    "    for idx, f in enumerate(neutral_frames):\n",
    "        for v in voiceless:\n",
    "            if f in v:\n",
    "                voiceless_frames.append(v)\n",
    "                if random.randint(0,1):\n",
    "                    control_frames.append(voiced_frames[idx])\n",
    "                    control_condition_order.append('voiced')\n",
    "                else:\n",
    "                    control_frames.append(v)\n",
    "                    control_condition_order.append('voiceless')\n",
    "        \n",
    "    return voiced_frames, voiceless_frames, control_frames, control_condition_order\n",
    "\n",
    "\n",
    "def create_trial_batch(condition, target_pair, frames, talker, n_steps, control_order=[]):\n",
    "    TARGET_ONSET_MS = 28000\n",
    "    SR = 16000\n",
    "    TARGET_IDX = int(28.0 * SR)\n",
    "    silence_500_np = np.zeros(int(0.5 * SR), dtype=np.float32)\n",
    "    \n",
    "    # Build shared context array and track WordBoundaries\n",
    "    context_blocks = []\n",
    "    context_boundaries = []\n",
    "    \n",
    "    # First, calculate total context length to determine initial padding offset\n",
    "    temp_len = 0\n",
    "    for idx, f in enumerate(frames):\n",
    "        c_type = control_order[idx] if condition == 'control' else condition\n",
    "        # calculate the padding; load_to_array is used for consistency\n",
    "        audio_len = len(load_to_array(f'audio/MP/{talker}/{c_type}/{f}'))\n",
    "        temp_len += audio_len + len(silence_500_np)\n",
    "        \n",
    "    pad_len = TARGET_IDX - temp_len\n",
    "    if pad_len < 0: raise ValueError(\"Context too long\")\n",
    "    \n",
    "    # Current pointer starts after the leading padding\n",
    "    current_sample_ptr = pad_len\n",
    "    \n",
    "    for idx, f in enumerate(frames):\n",
    "        c_type = control_order[idx] if condition == 'control' else condition\n",
    "        audio_np = load_to_array(f'audio/MP/{talker}/{c_type}/{f}')\n",
    "        \n",
    "        # Calculate boundaries for this word\n",
    "        start_sec = current_sample_ptr / SR\n",
    "        end_sec = (current_sample_ptr + len(audio_np)) / SR\n",
    "        label = re.search(r\"_([a-z]+).wav\", f).group(1)\n",
    "        context_boundaries.append(WordBoundary(start_sec, end_sec, label))\n",
    "        \n",
    "        context_blocks.append(audio_np)\n",
    "        context_blocks.append(silence_500_np)\n",
    "        current_sample_ptr += len(audio_np) + len(silence_500_np)\n",
    "    \n",
    "    context_np = np.concatenate(context_blocks)\n",
    "    padding = np.zeros(pad_len, dtype=np.float32)\n",
    "    full_context = np.concatenate([padding, context_np])\n",
    "\n",
    "    # 2. Stitch and create TrialData metadata\n",
    "    batch_arrays = []\n",
    "    trials_metadata = []\n",
    "    \n",
    "    for i in range(n_steps):\n",
    "        target_path = f'audio/MP/{talker}/continuum/{target_pair}_1_{i}.wav'\n",
    "        target_np = load_to_array(target_path)\n",
    "        \n",
    "        # Determine target label and boundaries\n",
    "        parts = target_pair.split(\"_\")\n",
    "        target_label = parts[0] if condition == 'voiceless' else parts[1]\n",
    "        t_start, t_end = 28.0, 28.0 + (len(target_np) / SR)\n",
    "        \n",
    "        # Combine shared context boundaries with the target boundary\n",
    "        all_boundaries = context_boundaries + [WordBoundary(t_start, t_end, target_label)]\n",
    "        \n",
    "        # Assemble 30s buffer\n",
    "        trial_buffer = np.zeros(30 * SR, dtype=np.float32)\n",
    "        trial_buffer[:TARGET_IDX] = full_context\n",
    "        trial_buffer[TARGET_IDX : TARGET_IDX + len(target_np)] = target_np\n",
    "        \n",
    "        batch_arrays.append(trial_buffer)\n",
    "        \n",
    "        # Create TrialData object (no audio attached here to save memory)\n",
    "        trial = TrialData(\n",
    "            transcript=\"\", \n",
    "            outcome=\"\",\n",
    "            word_boundaries=all_boundaries,\n",
    "            condition=condition,\n",
    "            continuum_step=i,\n",
    "            target_word=target_label,\n",
    "            target_array=target_np\n",
    "        )\n",
    "        trials_metadata.append(trial)\n",
    "        \n",
    "    return batch_arrays, full_context, trials_metadata\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fe7b383",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:28<00:00, 14.15s/it]\n"
     ]
    }
   ],
   "source": [
    "def selective_adaptation(output_path, talkers, n_continuum, n_trials):\n",
    "    all_experiment_data = []\n",
    "        \n",
    "    for t in talkers:\n",
    "        targets = os.listdir(f'audio/MP/{t}/continuum')\n",
    "        targets = list(set([re.search(r'([a-zA-Z]+_[a-zA-Z]+)_[0-9]', target).group(1) for target in targets]))\n",
    "        voiced = os.listdir(f'audio/MP/{t}/voiced')\n",
    "        voiced = [v for v in voiced if v != '.DS_Store']\n",
    "        voiceless = os.listdir(f'audio/MP/{t}/voiceless')\n",
    "        voiceless = [v for v in voiceless if v != '.DS_Store']\n",
    "        \n",
    "        for n in tqdm.tqdm(range(n_trials)):\n",
    "            target_pair = targets[n % len(targets)]\n",
    "            v_frames, vl_frames, c_frames, c_order = select_frames(target_pair, voiced, voiceless, t)\n",
    "\n",
    "            for c in ['voiced', 'voiceless', 'control']:\n",
    "                frames = v_frames if c == 'voiced' else vl_frames if c == 'voiceless' else c_frames\n",
    "                \n",
    "                # Create the batch of 13 stitched arrays\n",
    "                batch_audio, context_audio, metadata_list = create_trial_batch(c, target_pair, frames, t, n_continuum, c_order)\n",
    "                \n",
    "                # Transcribe entire continuum for this condition in one forward pass\n",
    "                transcripts = transcribe_batch(batch_audio, processor, model, device)\n",
    "\n",
    "                for i, text in enumerate(transcripts):\n",
    "                    metadata_list[i].transcript = text\n",
    "                    all_experiment_data.append({\n",
    "                        \"context\": context_audio, \n",
    "                        \"metadata\": metadata_list[i],\n",
    "                        \"target_pair\": target_pair,\n",
    "                        \"iteration\": n\n",
    "                    })\n",
    "\n",
    "    with open(output_path, 'wb') as file:\n",
    "        pickle.dump(data, file)\n",
    "\n",
    "    return all_experiment_data\n",
    "\n",
    "\n",
    "data = selective_adaptation('data/SelAd_test.csv', ['hope'], 13, 300)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "perc_learn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
